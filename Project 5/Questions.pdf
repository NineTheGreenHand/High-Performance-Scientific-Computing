%PDF-1.4
%“Œ‹ž ReportLab Generated PDF document http://www.reportlab.com
1 0 obj
<<
/F1 2 0 R /F2 3 0 R /F3 5 0 R
>>
endobj
2 0 obj
<<
/BaseFont /Helvetica /Encoding /WinAnsiEncoding /Name /F1 /Subtype /Type1 /Type /Font
>>
endobj
3 0 obj
<<
/BaseFont /Helvetica-Bold /Encoding /WinAnsiEncoding /Name /F2 /Subtype /Type1 /Type /Font
>>
endobj
4 0 obj
<<
/Contents 24 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 23 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 /Trans <<

>> 
  /Type /Page
>>
endobj
5 0 obj
<<
/BaseFont /Courier /Encoding /WinAnsiEncoding /Name /F3 /Subtype /Type1 /Type /Font
>>
endobj
6 0 obj
<<
/Contents 25 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 23 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 /Trans <<

>> 
  /Type /Page
>>
endobj
7 0 obj
<<
/Contents 26 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 23 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 /Trans <<

>> 
  /Type /Page
>>
endobj
8 0 obj
<<
/Contents 27 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 23 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 /Trans <<

>> 
  /Type /Page
>>
endobj
9 0 obj
<<
/Contents 28 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 23 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 /Trans <<

>> 
  /Type /Page
>>
endobj
10 0 obj
<<
/Outlines 12 0 R /PageLabels 29 0 R /PageMode /UseNone /Pages 23 0 R /Type /Catalog
>>
endobj
11 0 obj
<<
/Author () /CreationDate (D:20210510102130+00'00') /Creator (\(unspecified\)) /Keywords () /ModDate (D:20210510102130+00'00') /Producer (ReportLab PDF Library - www.reportlab.com) 
  /Subject (\(unspecified\)) /Title () /Trapped /False
>>
endobj
12 0 obj
<<
/Count 10 /First 13 0 R /Last 22 0 R /Type /Outlines
>>
endobj
13 0 obj
<<
/Dest [ 4 0 R /XYZ 62.69291 765.0236 0 ] /Next 14 0 R /Parent 12 0 R /Title (Question 1)
>>
endobj
14 0 obj
<<
/Dest [ 4 0 R /XYZ 62.69291 294.0236 0 ] /Next 15 0 R /Parent 12 0 R /Prev 13 0 R /Title (Question 2: pnorm)
>>
endobj
15 0 obj
<<
/Dest [ 6 0 R /XYZ 62.69291 627.0236 0 ] /Next 16 0 R /Parent 12 0 R /Prev 14 0 R /Title (Question 3: fnorm)
>>
endobj
16 0 obj
<<
/Dest [ 6 0 R /XYZ 62.69291 336.0236 0 ] /Next 17 0 R /Parent 12 0 R /Prev 15 0 R /Title (Question 4: cnorm)
>>
endobj
17 0 obj
<<
/Dest [ 7 0 R /XYZ 62.69291 729.0236 0 ] /Next 18 0 R /Parent 12 0 R /Prev 16 0 R /Title (Question 5: rnorm \(Amath 583 Only\))
>>
endobj
18 0 obj
<<
/Dest [ 7 0 R /XYZ 62.69291 582.0236 0 ] /Next 19 0 R /Parent 12 0 R /Prev 17 0 R /Title (Question 6: General)
>>
endobj
19 0 obj
<<
/Dest [ 7 0 R /XYZ 62.69291 279.0236 0 ] /Next 20 0 R /Parent 12 0 R /Prev 18 0 R /Title (Question 7: Conundrum #1)
>>
endobj
20 0 obj
<<
/Dest [ 8 0 R /XYZ 62.69291 651.0236 0 ] /Next 21 0 R /Parent 12 0 R /Prev 19 0 R /Title (Question 8 & 9 are not required in Questions.rst)
>>
endobj
21 0 obj
<<
/Dest [ 8 0 R /XYZ 62.69291 618.0236 0 ] /Next 22 0 R /Parent 12 0 R /Prev 20 0 R /Title (Question 10: Parallel matvec)
>>
endobj
22 0 obj
<<
/Dest [ 8 0 R /XYZ 62.69291 123.0236 0 ] /Parent 12 0 R /Prev 21 0 R /Title (Question 11: Conundrum #2)
>>
endobj
23 0 obj
<<
/Count 5 /Kids [ 4 0 R 6 0 R 7 0 R 8 0 R 9 0 R ] /Type /Pages
>>
endobj
24 0 obj
<<
/Length 7605
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 744.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Question 1) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 732.0236 cm
Q
q
1 0 0 1 62.69291 732.0236 cm
Q
q
1 0 0 1 62.69291 720.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (At what problem size do the answer between the computed norms start to differ?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 720.0236 cm
Q
q
1 0 0 1 62.69291 630.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 74 Tm /F1 10 Tf 12 TL .917485 Tw (Answer: I found out that the two_norm forward and two_norm reverse start to differ around the problem) Tj T* 0 Tw .77936 Tw (size 1100. For the two_norm forward and two_norm sorted, the norm starts to differ around the problem) Tj T* 0 Tw 2.055318 Tw (size 1600. However, I found out that for smaller problem sizes, sometimes the computed norms are) Tj T* 0 Tw .046098 Tw (different from the two-norm forward, but I think that for very small problem sizes, the result maybe not very) Tj T* 0 Tw .712485 Tw (insightful. For example, from my end, I have the two-norm forward differs from the two-norm reversed at) Tj T* 0 Tw .449983 Tw (problem size of 2. I got Absolute difference: 1.11022e-16 and Relative difference: 1.16342e-16, since the) Tj T* 0 Tw (differences are both on e-16, I think choose a larger problem size would be more useful.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 624.0236 cm
Q
q
1 0 0 1 62.69291 624.0236 cm
Q
q
1 0 0 1 62.69291 612.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (How do the absolute and relative errors change as a function of problem size?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 612.0236 cm
Q
q
1 0 0 1 62.69291 582.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .297126 Tw (Answer: For problem size 1100, for two-norm forward and two-norm reversed, I have Absolute difference:) Tj T* 0 Tw (7.10543e-15 and Relative difference: 1.89159e-16.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 552.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 2.884597 Tw (For problem size 11000, for two-norm forward and two-norm reversed, I have Absolute difference:) Tj T* 0 Tw (2.84217e-14 and Relative difference: 2.34869e-16.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 522.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 2.456905 Tw (For problem size 110000, for two-norm forward and two-norm reversed, I have Absolute difference:) Tj T* 0 Tw (5.68434e-14 and Relative difference: 1.48572e-16.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 492.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 2.029213 Tw (For problem size 1100000, for two-norm forward and two-norm reversed, I have Absolute difference:) Tj T* 0 Tw (2.50111e-12 and Relative difference: 2.06418e-15.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 450.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 1.059986 Tw (From the above examples, it is clear that as we increase the problem size, the absolute difference and) Tj T* 0 Tw .883735 Tw (relative difference also show a general increase trend. So, it seems like the absolute and relative errors) Tj T* 0 Tw (are directly proportional to the problem size.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 444.0236 cm
Q
q
1 0 0 1 62.69291 444.0236 cm
Q
q
1 0 0 1 62.69291 432.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Does the Vector class behave strictly like a member of an abstract vector class?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 432.0236 cm
Q
q
1 0 0 1 62.69291 366.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F1 10 Tf 12 TL .758651 Tw (Answer: For an abstract vector class, the two-norm reversed and two-norm sorted should be exactly the) Tj T* 0 Tw .457488 Tw (same as the two-norm forward no matter how large the problem size is as long as all the elements in the) Tj T* 0 Tw 2.241318 Tw (vector are included for once. However, in our case, as problem size increases, we have two-norms) Tj T* 0 Tw -0.01939 Tw (implemented in the different ways different from each other. So, the Vector class does NOT behave strictly) Tj T* 0 Tw (like a member of an abstract vector class.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 360.0236 cm
Q
q
1 0 0 1 62.69291 360.0236 cm
Q
q
1 0 0 1 62.69291 348.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Do you have any concerns about this kind of behavior?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 348.0236 cm
Q
q
1 0 0 1 62.69291 306.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 1.883555 Tw (Answer: For large-scale and complex computations, the errors we get for each loop/function by such) Tj T* 0 Tw 1.021235 Tw (vector computation would accumulate very fast so that the final result might be different as expected to) Tj T* 0 Tw (cause trouble.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 273.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Question 2: pnorm) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 261.0236 cm
Q
q
1 0 0 1 62.69291 261.0236 cm
Q
q
1 0 0 1 62.69291 249.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (What was the data race?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 249.0236 cm
Q
q
1 0 0 1 62.69291 207.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 1.052651 Tw (Answer: We passing sum into the helper function worker_a so that every thread accesses and updates) Tj T* 0 Tw 2.279983 Tw (sum before the previous thread finishes its process. This causes the following thread accesses and) Tj T* 0 Tw (updates from an incorrect partial sum caused by the previous threads.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 201.0236 cm
Q
q
1 0 0 1 62.69291 201.0236 cm
Q
q
1 0 0 1 62.69291 177.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .865542 Tw (What did you do to fix the data race? Explain why the race is actually eliminated \(rather than, say,) Tj T* 0 Tw (just made less likely\).) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 177.0236 cm
Q
q
1 0 0 1 62.69291 123.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL -0.019458 Tw (Answer: In the helper function worker_a, I created a double temp variable, set it equals to 0.0. Also, before) Tj T* 0 Tw .39561 Tw (the function worker_a I created a mutex called p_mutex. Inside the function worker_a, inside the for loop,) Tj T* 0 Tw .038488 Tw (instead of updating the partial variable, I have temp += x\(i\) * x\(i\); so that the partial variable is not changed) Tj T* 0 Tw (yet. Then I used Lock Guard to bloock scope:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 105.0236 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg ({ std::lock_guard) Tj (<) Tj (std::mutex) Tj (> p_guard\(p_mutex\);) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 99.02362 cm
Q
q
1 0 0 1 62.69291 87.02362 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 0 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (partial += temp;) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 87.02362 cm
Q
 
endstream
endobj
25 0 obj
<<
/Length 8144
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 753.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (}) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 711.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .391567 Tw (I am using the mutex Lock Guard to lock the updating process of the partial variable in the current thread) Tj T* 0 Tw .371163 Tw (to make sure the update is done properly and completely, then release the updated partial variable to the) Tj T* 0 Tw (next thread. With those updates, I can ensure the correctness of my sum update after each thread.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 705.0236 cm
Q
q
1 0 0 1 62.69291 705.0236 cm
Q
q
1 0 0 1 62.69291 693.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (How much parallel speedup do you see for 1, 2, 4, and 8 threads?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 693.0236 cm
Q
q
1 0 0 1 62.69291 639.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL 1.389398 Tw (Answer: For 1 thread, it's about 0.97 times speedup \(no speedup\); for 2 threads, it's about 1.63 times) Tj T* 0 Tw .799398 Tw (speedup; for 4 threads, it's about 1.96 times speedup; for 8 threads, it's also about 1.98 times speedup.) Tj T* 0 Tw .502927 Tw (We see that for 4 threads and 8 threads, the performance improvement is about the same, both showing) Tj T* 0 Tw (about 2 times the performance compared with the sequential performance.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 606.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Question 3: fnorm) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 594.0236 cm
Q
q
1 0 0 1 62.69291 594.0236 cm
Q
q
1 0 0 1 62.69291 582.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (How much parallel speedup do you see for 1, 2, 4, and 8 threads for ) Tj /F3 10 Tf (partitioned_two_norm_a) Tj /F1 10 Tf (?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 582.0236 cm
Q
q
1 0 0 1 62.69291 516.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F1 10 Tf 12 TL 1.389398 Tw (Answer: For 1 thread, it's about 0.96 times speedup \(no speedup\); for 2 threads, it's about 1.61 times) Tj T* 0 Tw .799398 Tw (speedup; for 4 threads, it's about 1.94 times speedup; for 8 threads, it's also about 1.97 times speedup.) Tj T* 0 Tw .679488 Tw (The result is very similar when we see the speedup for pnorm in the previous section. We see that for 4) Tj T* 0 Tw .49936 Tw (threads and 8 threads, the performance improvement is about the same, both showing about 2 times the) Tj T* 0 Tw (performance compared with the sequential performance.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 510.0236 cm
Q
q
1 0 0 1 62.69291 510.0236 cm
Q
q
1 0 0 1 62.69291 498.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F1 10 Tf 0 0 0 rg (How much parallel speedup do you see for 1, 2, 4, and 8 threads for ) Tj /F3 10 Tf (partitioned_two_norm_b) Tj /F1 10 Tf (?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 498.0236 cm
Q
q
1 0 0 1 62.69291 444.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL 1.389398 Tw (Answer: For 1 thread, it's about 0.98 times speedup \(no speedup\); for 2 threads, it's about 0.99 times) Tj T* 0 Tw .636457 Tw (speedup \(no speedup\); for 4 threads, it's about 0.99 times speedup \(no speedup\); for 8 threads, it's also) Tj T* 0 Tw -0.122868 Tw (about 0.97 times speedup \(no speedup\). We can see that clearly in this case, for 1, 2, 4, 8 threads, there is) Tj T* 0 Tw (no speedup compared with the sequential performance.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 438.0236 cm
Q
q
1 0 0 1 62.69291 438.0236 cm
Q
q
1 0 0 1 62.69291 414.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 14 Tm 16.15854 Tw 12 TL /F1 10 Tf 0 0 0 rg (Explain the differences you see between ) Tj /F3 10 Tf (partitioned_two_norm_a) Tj /F1 10 Tf ( and) Tj T* 0 Tw /F3 10 Tf (partitioned_two_norm_b) Tj /F1 10 Tf (.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 414.0236 cm
Q
q
1 0 0 1 62.69291 348.0236 cm
q
BT 1 0 0 1 0 50 Tm .126136 Tw 12 TL /F1 10 Tf 0 0 0 rg (Answer: For ) Tj /F3 10 Tf (partitioned_two_norm_a) Tj /F1 10 Tf (, we getting speedup result very close to pnorm where we used) Tj T* 0 Tw .158443 Tw /F3 10 Tf (thread) Tj /F1 10 Tf (, which makes sense. For ) Tj /F3 10 Tf (partitioned_two_norm_b) Tj /F1 10 Tf (, as thread number goes up, we don't see) Tj T* 0 Tw .265984 Tw (a speedup. This is very intuitive since for ) Tj /F3 10 Tf (partitioned_two_norm_b) Tj /F1 10 Tf ( we "deferred" the run until get\(\) is) Tj T* 0 Tw .146651 Tw (called. For ) Tj /F3 10 Tf (partitioned_two_norm_a) Tj /F1 10 Tf (, we have the program run right away and when we call get\(\), we) Tj T* 0 Tw (can simply get the results instead of just starting running the program.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 315.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Question 4: cnorm) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 303.0236 cm
Q
q
1 0 0 1 62.69291 303.0236 cm
Q
q
1 0 0 1 62.69291 291.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (How much parallel speedup do you see for 1, 2, 4, and 8 threads?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 291.0236 cm
Q
q
1 0 0 1 62.69291 201.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 74 Tm /F1 10 Tf 12 TL .401353 Tw (Answer: I used Cyclic Partitioning + Tasks but from the result I got, there is no speedup for 1, 2, 4, and 8) Tj T* 0 Tw 2.09811 Tw (threads compared with the sequential performance. I have passed all the test cases, but still all the) Tj T* 0 Tw 2.68811 Tw (numbers I am getting is lower than the sequential performance reading. So, I went ahead tried to) Tj T* 0 Tw .048876 Tw (implement the Cyclic Partitioning + Threads, but it seems like the outcome is very similar. For 1 thread, it's) Tj T* 0 Tw .275251 Tw (about 0.33 times the speedup; for 2 thread, it's about 0.54 times the speedup; for 4 thread, it's about 0.68) Tj T* 0 Tw 1.370574 Tw (times the speedup; for 8 threads, it's about 0.48 times the speedup. We see that by implementing the) Tj T* 0 Tw (Cyclic Partitioning, we actually getting slower performance.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 195.0236 cm
Q
q
1 0 0 1 62.69291 195.0236 cm
Q
q
1 0 0 1 62.69291 171.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 3.291647 Tw (How does the performance of cyclic partitioning compare to blocked? Explain any significant) Tj T* 0 Tw (differences, referring to, say, performance models or CPU architectural models.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 171.0236 cm
Q
q
1 0 0 1 62.69291 81.02362 cm
q
0 0 0 rg
BT 1 0 0 1 0 74 Tm /F1 10 Tf 12 TL .66881 Tw (Answer: The overall performance of cyclic partitioning is lower than blocked. It is worth to notice that the) Tj T* 0 Tw 2.114597 Tw (cyclic partitioning performance is even lower than the sequential performance. We are getting worse) Tj T* 0 Tw 1.090751 Tw (performance when use cyclic partitioning. We are getting very similar performance for block partitioning) Tj T* 0 Tw 1.570651 Tw (with threads and tasks with both getting about 2 times the performance compared with the sequential) Tj T* 0 Tw .227765 Tw (performance. This might because for block partitioning, we try to fetch the data next to each other, so that) Tj T* 0 Tw .755542 Tw (we can access the data we want directly from the cache. However, for cyclic partitioning, we try to fetch) Tj T* 0 Tw .332619 Tw (the data that are several steps away from each other, so that we lose the benefit of fetching them directly) Tj T* 0 Tw ET
Q
Q
 
endstream
endobj
26 0 obj
<<
/Length 7337
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 741.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .868626 Tw (from cache L1 or L2 since they may not be in there. We might need extra time to get the data we want) Tj T* 0 Tw (from higher level caches such as L3, L4 or even from RAM, which cause more time.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 708.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Question 5: rnorm \(Amath 583 Only\)) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 696.0236 cm
Q
q
1 0 0 1 62.69291 696.0236 cm
Q
q
1 0 0 1 62.69291 684.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (How much parallel speedup do you see for 1, 2, 4, and 8 threads?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 684.0236 cm
Q
q
1 0 0 1 62.69291 666.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Answer: This is for Amath 583 Only, I am an Amath 483 student.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 660.0236 cm
Q
q
1 0 0 1 62.69291 660.0236 cm
Q
q
1 0 0 1 62.69291 612.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 33 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 38 Tm .078976 Tw 12 TL /F1 10 Tf 0 0 0 rg (What will happen if you use ) Tj /F3 10 Tf (std:::launch::deferred) Tj /F1 10 Tf ( instead of ) Tj /F3 10 Tf (std:::launch::async) Tj /F1 10 Tf ( when) Tj T* 0 Tw 3.379213 Tw (launching tasks? When will the computations happen? Will you see any speedup? For your) Tj T* 0 Tw .056647 Tw (convenience, the driver program will also call ) Tj /F3 10 Tf (recursive_two_norm_b) Tj /F1 10 Tf ( -- which you can implement) Tj T* 0 Tw (as a copy of ) Tj /F3 10 Tf (recursive_two_norm_a) Tj /F1 10 Tf ( but with the launch policy changed.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 612.0236 cm
Q
q
1 0 0 1 62.69291 594.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Answer: This is for Amath 583 only, I am an Amath 483 student.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 561.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Question 6: General) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 549.0236 cm
Q
q
1 0 0 1 62.69291 549.0236 cm
Q
q
1 0 0 1 62.69291 525.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .082126 Tw (For the different approaches to parallelization, were there any major differences in how much parallel) Tj T* 0 Tw (speedup that you saw?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 525.0236 cm
Q
q
1 0 0 1 62.69291 471.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL .733516 Tw (Answer: For my fnorm and pnorm, I got very close results. Both approaches achieved about 2 times the) Tj T* 0 Tw .943059 Tw (performance compared with the sequential performance. But, my cnorm's results are very different from) Tj T* 0 Tw 3.212485 Tw (my fnorm and pnorm results as for my cnorm the results are actually worse than the sequential) Tj T* 0 Tw (performance. The reason for this have already been included in Question 4.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 465.0236 cm
Q
q
1 0 0 1 62.69291 465.0236 cm
Q
q
1 0 0 1 62.69291 417.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 33 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL .635542 Tw (You may have seen the speedup slowing down as the problem sizes got larger -- if you didn't keep) Tj T* 0 Tw .176905 Tw (trying larger problem sizes. What is limiting parallel speedup for two_norm \(regardless of approach\)?) Tj T* 0 Tw 2.268443 Tw (What would determine the problem sizes where you should see ideal speedup? \(Hint: Roofline) Tj T* 0 Tw (model.\)) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 417.0236 cm
Q
q
1 0 0 1 62.69291 291.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 110 Tm /F1 10 Tf 12 TL -0.074512 Tw (Answer: Since we need the data load into cache to let threads fetch the data and solve the problem, as the) Tj T* 0 Tw .012765 Tw (problem sizes increases, we would have more and more data that we need to load them into cache. When) Tj T* 0 Tw .955488 Tw (to the point where the problem size is too large to load the majority data into the cache, no matter how) Tj T* 0 Tw .371988 Tw (many thread we are using, the algorithm will not speed up by much since thread does not help us to load) Tj T* 0 Tw .24856 Tw (data into cache, it only solves the problem with the data that is already in the cache. If the problem size is) Tj T* 0 Tw .300465 Tw (so large that the cache if full, no matter how many thread we have, we cannot speed up. So, the cache is) Tj T* 0 Tw 1.742209 Tw (the key element to limit parallel speedup for two_norm, and it is also the key factor to determine the) Tj T* 0 Tw .374104 Tw (problem sizes where you should see ideal speedup. We can see this with a Roofline model provided with) Tj T* 0 Tw 1.372651 Tw (this assignment with instructor's computer: as when we have more threads, our RAM does not always) Tj T* 0 Tw (scale with the number of threads we have.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 258.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Question 7: Conundrum #1) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 246.0236 cm
Q
q
1 0 0 1 62.69291 246.0236 cm
Q
q
1 0 0 1 62.69291 234.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (1.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (What is causing this behavior?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 234.0236 cm
Q
q
1 0 0 1 62.69291 84.02362 cm
q
0 0 0 rg
BT 1 0 0 1 0 134 Tm /F1 10 Tf 12 TL -0.082367 Tw (Answer: When the problem size is small, our computer is already capable to solve the problem fast without) Tj T* 0 Tw 1.192339 Tw (parallelization. The reason for parallelization is that when we have a very large problem size, we try to) Tj T* 0 Tw .813876 Tw (have each thread/task to do part of the work so that the overall performance and execution time can be) Tj T* 0 Tw 1.154987 Tw (optimized. However, in order to achieve parallelization, we need to add and modify the code a bit so it) Tj T* 0 Tw 1.731751 Tw (works that way. In this assignment, for example, I used Block + Threads with Lock Guard \(mutex\) in) Tj T* 0 Tw 4.23498 Tw (pnorm.hpp to achieve parallelization. For large problem size, this modification can enhance the) Tj T* 0 Tw .963828 Tw (performance. However, for smaller problem sizes, when we use parallelization, everytime when we lock) Tj T* 0 Tw .537488 Tw (and unlock the lock guard, and moves to the next thread would take a large part of the execution time. It) Tj T* 0 Tw .587209 Tw (appears that it's not worth the work for a smaller problem size to use parallelization because the time for) Tj T* 0 Tw .258735 Tw (those extra work might ended up with taking more time than just solve the problem without parallelization.) Tj T* 0 Tw .498409 Tw (This is why this command is running so slow. I waited for 10 minutes, and it's still not done. \(./pnorm.exe) Tj T* 0 Tw (128 256\)) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 78.02362 cm
Q
q
1 0 0 1 62.69291 78.02362 cm
Q
 
endstream
endobj
27 0 obj
<<
/Length 7083
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 753.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (2.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (How could this behavior be fixed?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 753.0236 cm
Q
q
1 0 0 1 62.69291 711.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .276655 Tw (Answer: When we try to solve a problem, we can write code \(e.g. if loop\) to check the problem size. If the) Tj T* 0 Tw .415542 Tw (problem size is large, we use the parallelized version of function to solve the problem; if the problem size) Tj T* 0 Tw (is small, we can just solve the problem directly without parallelization.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 705.0236 cm
Q
q
1 0 0 1 62.69291 705.0236 cm
Q
q
1 0 0 1 62.69291 693.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (3.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Is there a simple implementation for this fix?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 693.0236 cm
Q
q
1 0 0 1 62.69291 663.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .65683 Tw (Answer: I think so, but we are not required to write it down from the assignment instruction. We are only) Tj T* 0 Tw (required to do 1 and 2 based on the assignment instruction.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 630.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Question 8 & 9 are not required in Questions.rst) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 597.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Question 10: Parallel matvec) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 585.0236 cm
Q
q
1 0 0 1 62.69291 585.0236 cm
Q
q
1 0 0 1 62.69291 573.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Which methods did you implement?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 573.0236 cm
Q
q
1 0 0 1 62.69291 519.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL 4.230751 Tw (Answer: I used Block + Thread approach when implementing the matvec\(\), and t_matvec\(\) for) Tj T* 0 Tw 1.30152 Tw (CSRMatrix.hpp and CSCMatrix.hpp. For the four functions implemented, I have one helper function for) Tj T* 0 Tw 1.09061 Tw (each of them. The four overloaded functions are implemented in a way such that parallelization can be) Tj T* 0 Tw (achieved.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 513.0236 cm
Q
q
1 0 0 1 62.69291 513.0236 cm
Q
q
1 0 0 1 62.69291 489.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.419987 Tw (How much parallel speedup do you see for the methods that you implemented for 1, 2, 4, and 8) Tj T* 0 Tw (threads?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 489.0236 cm
Q
q
1 0 0 1 62.69291 471.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Answer: If we take COO as the sequential performance for comparsion, then:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 381.0236 cm
q
BT 1 0 0 1 0 74 Tm .570514 Tw 12 TL /F1 10 Tf 0 0 0 rg (For CSR matvec \(compared with COO\): For 1 thread with grid size N <) Tj (= 512, I am getting lower reading) Tj T* 0 Tw .277209 Tw (for CSR compared with COO, but for larger N >) Tj (= 1024, it's about 1.25 times speedup in the performance.) Tj T* 0 Tw .276488 Tw (For 2 threads, for grid size N >) Tj (= 256, it's about 1.77 times speedup in the performance. For 4 threads, for) Tj T* 0 Tw 1.180988 Tw (grid size N >) Tj (= 256, it's about 1.95 times speedup in the performance. For 8 threads, for N >) Tj (= 512, it's) Tj T* 0 Tw .918735 Tw (about 2.05 times speedup in the performance. So overall, for CSR matvec with parallelization, with 4 or) Tj T* 0 Tw .327045 Tw (more threads and larger grid size, it's about 2 times the speedup in the performance compared with COO) Tj T* 0 Tw (matvec.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 303.0236 cm
q
BT 1 0 0 1 0 62 Tm .69832 Tw 12 TL /F1 10 Tf 0 0 0 rg (For CSR t_matvec \(compared with COO^T\): For 1 thread with grid size N >) Tj (= 2048, it's about 1.01 times) Tj T* 0 Tw 3.135868 Tw (speedup in the performance. For 2 threads with N >) Tj (= 256, it's about 1.73 times speedup in the) Tj T* 0 Tw -0.125013 Tw (performance. For 4 threads with N >) Tj (= 256, it's about 2.29 times speedup in the performance. For 8 threads) Tj T* 0 Tw 1.795868 Tw (with N >) Tj (= 512, it's about 2.13 times speedup in the performance. We see that the best performance) Tj T* 0 Tw .573555 Tw (speedup achieved by CSR t_matvec with parallelization is about 2.3 times speedup compared with COO) Tj T* 0 Tw (t_matvec.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 213.0236 cm
q
BT 1 0 0 1 0 74 Tm .014988 Tw 12 TL /F1 10 Tf 0 0 0 rg (For CSC matvec \(compared with COO\): For 1 thread with grid size N <) Tj (= 2048 \(all grid size provided\), I am) Tj T* 0 Tw .111984 Tw (getting lower reading for CSC compareed with COO. It's about 0.86 times the performance compared with) Tj T* 0 Tw .29104 Tw (COO \(no speedup\). For 2 threads with N >) Tj (= 256, it's about 1.32 times speedup in the performance. For 4) Tj T* 0 Tw .180488 Tw (threads with N >) Tj (= 256, it's about 1.93 times speedup in the performance. For 8 threads with N >) Tj (= 512, it's) Tj T* 0 Tw .918735 Tw (about 1.95 times speedup in the performance. So overall, for CSC matvec with parallelization, with 4 or) Tj T* 0 Tw .327045 Tw (more threads and larger grid size, it's about 2 times the speedup in the performance compared with COO) Tj T* 0 Tw (matvec.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 135.0236 cm
q
BT 1 0 0 1 0 62 Tm .69832 Tw 12 TL /F1 10 Tf 0 0 0 rg (For CSC t_matvec \(compared with COO^T\): For 1 thread with grid size N >) Tj (= 1024, it's about 1.31 times) Tj T* 0 Tw .699988 Tw (speedup in the performance. For 2 threads with grid size N >) Tj (= 256, it's about 1.97 times speedup in the) Tj T* 0 Tw .122765 Tw (performance. For 4 threads with grid size N >) Tj (= 256, it's about 2.31 times speedup in the performance. For) Tj T* 0 Tw .525988 Tw (8 threads with grid size N >) Tj (= 512, it's about 2.0 times speedup in the performance. We see that the best) Tj T* 0 Tw 3.53998 Tw (performance speedup achieved by CSC t_matvec with parallelization is about 2.3 times speedup) Tj T* 0 Tw (compared with COO t_matvec.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 102.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Question 11: Conundrum #2) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 90.02362 cm
Q
q
1 0 0 1 62.69291 90.02362 cm
Q
q
1 0 0 1 62.69291 78.02362 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (1.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (What are the two "matrix vector" operations that we could use?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 78.02362 cm
Q
 
endstream
endobj
28 0 obj
<<
/Length 3189
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 729.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 3.824597 Tw (Answer: First, we could use CSR::t_matvec. Second, we could use the CSC::matvec instead of) Tj T* 0 Tw 1.228735 Tw (CSR::matvec. In my pagerank.hpp, I used the second method, and the CSC::matvec I used is the one) Tj T* 0 Tw (overloaded with the third parameter being the number of threads \(partitions\) to achieve parallelization.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 723.0236 cm
Q
q
1 0 0 1 62.69291 723.0236 cm
Q
q
1 0 0 1 62.69291 699.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (2.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.375251 Tw (How would we use the first in pagerank? I.e., what would we have to do differently in the rest of) Tj T* 0 Tw (pagerank.cpp to use that first operation?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 699.0236 cm
Q
q
1 0 0 1 62.69291 633.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F1 10 Tf 12 TL 1.650651 Tw (Answer: First is we use CSR::t_matvec. We can modify the CSRMatrix.hpp \(which we already did for) Tj T* 0 Tw 1.99311 Tw (To-Do 8\) to have a parallelized version of t_matvec. Then, we can change the pagerank.hpp to use) Tj T* 0 Tw .570651 Tw (CSR::t_matvec. We do not have to call the transpose matrix product to compute the transposed product,) Tj T* 0 Tw 2.057882 Tw (instead, We just need to send in the transposed matrix to the call. There is no need to change the) Tj T* 0 Tw (pagerank.cpp and pagerank_test.cpp with this operation.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 627.0236 cm
Q
q
1 0 0 1 62.69291 627.0236 cm
Q
q
1 0 0 1 62.69291 615.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (3.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (How would we use the second?) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 615.0236 cm
Q
q
1 0 0 1 62.69291 501.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 98 Tm /F1 10 Tf 12 TL .929398 Tw (Answer: Second is we use CSC::matvec \(this is the one I used and modified in pagerank.hpp\). We can) Tj T* 0 Tw .64061 Tw (use the one with parallelization in CSCMatrix.hpp \(we did this for To-Do 8\). In pagerank.hpp, we change) Tj T* 0 Tw 1.796651 Tw (the CSR::matvec to CSC:matvec, and when we call mult\(\), we used the overloaded version wih third) Tj T* 0 Tw 5.529147 Tw (parameter being the num_threads. For this operation, we need to modify pagerank.cpp and) Tj T* 0 Tw .105366 Tw (pagerank_test.cpp in placed where CSRMatrix was used into CSCMatrix. \(Please see detailed changes in) Tj T* 0 Tw .302846 Tw (the files that I submitted.\) We have to change this or the testing files will try to keep using the CSR matrix) Tj T* 0 Tw -0.050013 Tw (which would cause error. \(Since we need to change the cpp and test file, when submitted to Gradescope, I) Tj T* 0 Tw .473516 Tw (commented out this modification since it will fail the Make command since I assume the test file provided) Tj T* 0 Tw (in Gradescope still try to read a CSR matrix instead of CSC matrix.\)) Tj T* ET
Q
Q
 
endstream
endobj
29 0 obj
<<
/Nums [ 0 30 0 R 1 31 0 R 2 32 0 R 3 33 0 R 4 34 0 R ]
>>
endobj
30 0 obj
<<
/S /D /St 1
>>
endobj
31 0 obj
<<
/S /D /St 2
>>
endobj
32 0 obj
<<
/S /D /St 3
>>
endobj
33 0 obj
<<
/S /D /St 4
>>
endobj
34 0 obj
<<
/S /D /St 5
>>
endobj
xref
0 35
0000000000 65535 f 
0000000073 00000 n 
0000000124 00000 n 
0000000231 00000 n 
0000000343 00000 n 
0000000548 00000 n 
0000000653 00000 n 
0000000858 00000 n 
0000001063 00000 n 
0000001268 00000 n 
0000001473 00000 n 
0000001579 00000 n 
0000001837 00000 n 
0000001912 00000 n 
0000002023 00000 n 
0000002154 00000 n 
0000002285 00000 n 
0000002416 00000 n 
0000002566 00000 n 
0000002699 00000 n 
0000002837 00000 n 
0000002999 00000 n 
0000003141 00000 n 
0000003267 00000 n 
0000003351 00000 n 
0000011008 00000 n 
0000019204 00000 n 
0000026593 00000 n 
0000033728 00000 n 
0000036969 00000 n 
0000037046 00000 n 
0000037080 00000 n 
0000037114 00000 n 
0000037148 00000 n 
0000037182 00000 n 
trailer
<<
/ID 
[<b6031ba99e21a9da762e5ad4a5b64599><b6031ba99e21a9da762e5ad4a5b64599>]
% ReportLab generated PDF document -- digest (http://www.reportlab.com)

/Info 11 0 R
/Root 10 0 R
/Size 35
>>
startxref
37216
%%EOF
